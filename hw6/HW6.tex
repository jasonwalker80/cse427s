\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[margin=0.5in,includehead,includefoot]{geometry}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{fancyhdr}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
}

\pagestyle{fancy}
\renewcommand{\headrulewidth}{0pt} % no line in header area
\lhead{Names: Bob Skowron, Jason Walker\\
Keys: rskowron, jwalker\\
SVN: jwalker: \url{https://svn.seas.wustl.edu/repositories/jwalker/cse427s_fl17/}\\}% Right header

\urlstyle{same}
\renewcommand{\labelitemiii}{$\bullet$}

\begin{document}
\begin{itemize}
\item[1.] 
	\begin{itemize}
		\item[a.] See SVN
		\item[b.] 
		%hadoop jar aggregateratings.jar stubs.AggregateRating -file=movie_details.txt netflix/TestingRatings.txt netflix/testing_output
		%hadoop jar TopNList.jar stubs.TopNDriver -D N=3 netflix/testing_output/part-r-00000 netflix/testing_topn_output
		%hadoop jar aggregateratings.jar stubs.AggregateRatings -files=movie_titles.txt netflix/TrainingRatings.txt netflix/training_output
		%hadoop jar TopNList.jar stubs.TopNDriver -D N=15 netflix/training_output/part-r-00000 netflix/training_topn_output
		%hadoop fs -cat netflix/training_topn_output/part-r-00000
		103701	Ferris Bueller's Day Off\\
		95216	Rain Man\\
		94398	Seven\\
		92377	The Godfather\\
		92029	The Incredibles\\
		90891	Pretty Woman\\
		88670	As Good as It Gets\\
		82862	The Italian Job\\
		81889	Terminator 2: Extreme Edition\\
		78936	When Harry Met Sally\\
		78892	National Lampoon's Vacation\\
		76587	Beverly Hills Cop\\
		76473	Office Space\\
		75145	Air Force One\\
		70400	Sweet Home Alabama
	\end{itemize}

\pagebreak
\item[2.]
	\begin{itemize}
		\item[a.] job.setCombinerClass(SumReducer.class);\\
			No, the output does not change.
		\item[b.] 
			\begin{tabular}{|c||c|c|}
				\hline 
				\textbf{Counter} & \textbf{With Combiner} & \textbf{Without Combiner}\\
				FILE: number of bytes read & 99,540 & 156,981,268\\
				FILE: number of bytes written & 373,984 & 236,309,496\\
				HDFS: number of bytes read & 58,524,136 & 58,524,136\\
				HDFS: number of bytes written & 47,019 & 47,019\\
				\hline
			\end{tabular}
		\\		
		The number of bytes written and read from HDFS do not change because the data read from HDFS (namely the TrainingRatings dataset) is the same in both cases. They both need to read the entire set and regardless of using the Combiner or not, we get the same answer and thus the same output bytes. The bytes written and read though on the filesystem do change, and are reduced with the Combiner. This is because the Combiner reduces the amount of intermediate data produced by the mappers by collapsing all the Mapper output from a given compute node before writing the intermediate data.\\
		Number of key-value pairs combined: 3,192,295 - 1,789 = 3,190,506d		
		\item[c.] We can use the SumReducer as the Combiner because:
			\item Our reduce step is summation, which is commutative and associative
			%\item The input data types of the Combiner and Reducer are the same (Text, IntWritable)
			\item The output data types of the Combiner and Mapper are the same (Text, IntWritable)
		\item[d.] A Reducer that finds the median would not be able to be used as the Combiner because it is not associative
	
	\end{itemize}

	
\end{itemize}

\end{document}
